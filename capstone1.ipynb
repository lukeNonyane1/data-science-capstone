{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective: identify potential monthly mortgage expenses for each region based on factors which are primarily monthly family income and rented value of the real estate.\n",
    "- predict the potential demand in dollars amount of loan for each of the region in the USA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# required packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib \n",
    "import matplotlib.pyplot as plt\n",
    "import altair as alt\n",
    "alt.data_transformers.disable_max_rows()\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#pip install pandasql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Data Science project stages\n",
    "1. Business understanding\n",
    "2. Data understanding\n",
    "3. Data preparation\n",
    "4. Modeling\n",
    "5. Evaluation\n",
    "6. Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. Data understanding\n",
    "- Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('test.csv')\n",
    "train_df = pd.read_csv('train.csv')\n",
    "test_df.shape, train_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3. Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Analyze Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Handling Row Duplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for duplicates on train set\n",
    "print(train_df.duplicated().sum())\n",
    "print(train_df.duplicated(subset=['state', 'city']).sum())\n",
    "\n",
    "# check for duplicates on test set\n",
    "print(test_df.duplicated().sum())\n",
    "print(test_df.duplicated(subset=['state', 'city']).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By looking only at these two columns instead of the entire column set, we can see that the number of duplicate rows has increased. This means that there are rows that have the exact same values as these two columns but have different values in other columns, which means they may be different records. It is better to use all columns to identity duplicate records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use UID as index\n",
    "\n",
    "# train set\n",
    "train_df.set_index(keys=['UID'], inplace=True)\n",
    "\n",
    "# test set\n",
    "test_df.set_index(keys=['UID'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# drop duplicates in order to avoid overfitting bias on model.\n",
    "\n",
    "# test set\n",
    "train_df = train_df.drop_duplicates(keep='first')\n",
    "print(train_df.shape)\n",
    "\n",
    "# test set\n",
    "test_df = test_df.drop_duplicates(keep='first')\n",
    "print(test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# check for null values\n",
    "\n",
    "# train set\n",
    "print(train_df.isna().sum().sum())\n",
    "\n",
    "# test set\n",
    "print(test_df.isna().sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Converting data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train set \n",
    "train_obj_df = train_df.select_dtypes(include='object')\n",
    "\n",
    "# test set\n",
    "test_obj_df = test_df.select_dtypes(include='object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# list of unique values for each variable of the 'object' type\n",
    "\n",
    "# train set\n",
    "train_obj_cols = train_obj_df.columns\n",
    "#for i in train_obj_cols:\n",
    "    #print(i)\n",
    "    #print(train_obj_df[i].unique())\n",
    "    \n",
    "# test set\n",
    "test_obj_cols = train_obj_df.columns\n",
    "#for i in test_obj_cols:\n",
    "    #print(i)\n",
    "    #print(test_obj_df[i].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All these columns have a finite number of unique values that are composed of text, which shows that they are categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train set: convert to object from numerical\n",
    "train_vars_convert = ['COUNTYID', 'STATEID', 'zip_code', 'area_code']\n",
    "for i in train_vars_convert:\n",
    "    train_df[i] = train_df[i].astype('object')\n",
    "print(train_df.select_dtypes(include='object').head())\n",
    "\n",
    "# test set: convert to object from numerical\n",
    "test_vars_convert = ['COUNTYID', 'STATEID', 'zip_code', 'area_code']\n",
    "for i in test_vars_convert:\n",
    "    test_df[i] = test_df[i].astype('object')\n",
    "print(test_df.select_dtypes(include='object').head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Features to drop\n",
    "- Theses features only have 1 value so they do not add any variance to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train set\n",
    "train_features_to_drop = []\n",
    "for column in train_df:\n",
    "    if (train_df[column].nunique() == 1):\n",
    "        train_features_to_drop.append(column)\n",
    "print(train_features_to_drop)\n",
    "\n",
    "# test set\n",
    "test_features_to_drop = []\n",
    "for column in test_df:\n",
    "    if (test_df[column].nunique() == 1):\n",
    "        test_features_to_drop.append(column)\n",
    "print(test_features_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping features with nunique() = 1 and BLOCKID; [has no values]\n",
    "\n",
    "# train set\n",
    "train_df.drop(columns=['SUMLEVEL', 'primary', 'BLOCKID'], inplace=True)\n",
    "print(train_df.shape)\n",
    "\n",
    "# test set\n",
    "test_df.drop(columns=['SUMLEVEL', 'primary', 'BLOCKID'], inplace=True)\n",
    "print(test_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Missing value treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# check for train set missing values\n",
    "print('Train missing values total: {}'.format(train_df.isna().sum().sum()))\n",
    "\n",
    "# check for test set missing values\n",
    "print('Test missing values total: {}'.format(test_df.isna().sum().sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# list of train set columns with missing values\n",
    "train_cols_missing_vals = []\n",
    "for i in train_df.columns:\n",
    "    if (train_df[i].isna().sum() >= 1):\n",
    "        train_cols_missing_vals.append(i)\n",
    "        \n",
    "# list of test set columns with missing values\n",
    "test_cols_missing_vals = []\n",
    "for i in test_df.columns:\n",
    "    if (test_df[i].isna().sum() >= 1):\n",
    "        test_cols_missing_vals.append(i)\n",
    "        \n",
    "print(len(train_cols_missing_vals), len(test_cols_missing_vals))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Missing value treatment for categorical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# missing categorical values train set\n",
    "train_missing_cat_vars = []\n",
    "for i in train_cols_missing_vals:\n",
    "    if ((train_df[i].dtype == 'object')):\n",
    "        train_missing_cat_vars.append(i)\n",
    "\n",
    "\n",
    "# missing categorical values test set\n",
    "test_missing_cat_vars = []\n",
    "for i in test_cols_missing_vals:\n",
    "    if ((test_df[i].dtype == 'object')):\n",
    "        test_missing_cat_vars.append(i)\n",
    "\n",
    "print(len(train_missing_cat_vars), len(test_missing_cat_vars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There aren't any missing values for categorical features. We will confirm by checking the df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('Train & Test missing categorical values:{} \\n'.format(train_df.select_dtypes(include=('object', 'category')).isna().sum()))\n",
    "print(test_df.select_dtypes(include=('object', 'category')).isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Missing value treatment for numerical columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train set missing numerical values \n",
    "train_missing_num_vars = []\n",
    "for i in train_cols_missing_vals:\n",
    "    if (train_df[i].dtype != 'object'):\n",
    "        train_missing_num_vars.append(i)\n",
    "#train_missing_num_vars\n",
    "\n",
    "# test set missing numerical values \n",
    "test_missing_num_vars = []\n",
    "for i in test_cols_missing_vals:\n",
    "    if (test_df[i].dtype != 'object'):\n",
    "        test_missing_num_vars.append(i)\n",
    "#test_missing_num_vars\n",
    "\n",
    "print('Train & Test missing values for numerical columns respectively: {} {}'.format(len(train_missing_num_vars), len(test_missing_num_vars)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Distribution of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train_skewness_df\n",
    "train_skewness_df = pd.DataFrame()\n",
    "for i in train_missing_num_vars:\n",
    "    train_skewness_df[i] = train_df[i]\n",
    "    \n",
    "# test_skewness_df\n",
    "test_skewness_df = pd.DataFrame()\n",
    "for i in test_missing_num_vars:\n",
    "    test_skewness_df[i] = test_df[i]\n",
    "#test_skewness_df.head()\n",
    "\n",
    "train_skewness_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train set column list\n",
    "train_num_cols_with_na = train_skewness_df.keys().tolist()\n",
    "print(len(train_num_cols_with_na))\n",
    "\n",
    "# test set column list\n",
    "test_num_cols_with_na = test_skewness_df.keys().tolist()\n",
    "print(len(test_num_cols_with_na))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train set value list\n",
    "train_num_skeweness_values = []\n",
    "for i in train_num_cols_with_na:\n",
    "    train_num_skeweness_values.append(train_df[i].skew())\n",
    "print(len(train_num_skeweness_values))\n",
    "\n",
    "# test set value list\n",
    "test_num_skeweness_values = []\n",
    "for i in test_num_cols_with_na:\n",
    "    test_num_skeweness_values.append(test_df[i].skew())\n",
    "print(len(test_num_skeweness_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train set absolute values\n",
    "train_num_skeweness_values = [abs(i) for i in train_num_skeweness_values]\n",
    "#train_num_skeweness_values\n",
    "\n",
    "# test set absolute values\n",
    "test_num_skeweness_values = [abs(i) for i in train_num_skeweness_values]\n",
    "#test_num_skeweness_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_skewness_df = pd.DataFrame({'columns':train_num_cols_with_na, 'skew_value':train_num_skeweness_values})\n",
    "train_skewness_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_skewness_df = pd.DataFrame({'columns':test_num_cols_with_na, 'skew_value':test_num_skeweness_values})\n",
    "test_skewness_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(test_num_cols_with_na))\n",
    "print(len(test_num_skeweness_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create skewness bins\n",
    "# symmetric = < 0.5\n",
    "# slighty_skewed  = 0.5 - 1\n",
    "# highly_skewed = > 1\n",
    "train_skeweness_labels = ['symmetric','slighty_skewed', 'highly_skewed']\n",
    "train_skeweness_bins = [0, 0.5, 1, float('inf')]\n",
    "train_skewness_df['skew_bin'] = pd.cut(train_skewness_df['skew_value'], bins=train_skeweness_bins, labels=train_skeweness_labels)\n",
    "\n",
    "# test set\n",
    "test_skeweness_labels = ['symmetric','slighty_skewed', 'highly_skewed']\n",
    "test_skeweness_bins = [0, 0.5, 1, float('inf')]\n",
    "test_skewness_df['skew_bin'] = pd.cut(test_skewness_df['skew_value'], bins=test_skeweness_bins, labels=test_skeweness_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_skewness_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_skewness_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# symmetric numerical columns with missing values.\n",
    "# Missing values will be replaced with mean.\n",
    "# train set\n",
    "train_symmetric_num_cols = train_skewness_df.loc[train_skewness_df['skew_bin'] == 'symmetric']\n",
    "train_symmetric_num_cols = train_symmetric_num_cols['columns'].to_list()\n",
    "print(len(train_symmetric_num_cols))\n",
    "\n",
    "# test set\n",
    "test_symmetric_num_cols = test_skewness_df.loc[test_skewness_df['skew_bin'] == 'symmetric']\n",
    "test_symmetric_num_cols = test_symmetric_num_cols['columns'].to_list()\n",
    "print(len(test_symmetric_num_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# asymmetric numerical columns with missing values.\n",
    "# Missing values will be replaced with median.\n",
    "# train set\n",
    "train_asymmetric_num_cols = train_skewness_df.loc[train_skewness_df['skew_bin'] != 'symmetric']\n",
    "train_asymmetric_num_cols = train_asymmetric_num_cols['columns'].to_list()\n",
    "print(len(train_asymmetric_num_cols))\n",
    "\n",
    "# test set\n",
    "test_asymmetric_num_cols = test_skewness_df.loc[test_skewness_df['skew_bin'] != 'symmetric']\n",
    "test_asymmetric_num_cols = test_asymmetric_num_cols['columns'].to_list()\n",
    "print(len(test_asymmetric_num_cols))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# function to extract asymmetric/symmetric columns which takes skew_bin value as input parameter\n",
    "# skew_bin_value can be any in the following list ['symmetric','slighty_skewed', 'highly_skewed']\n",
    "train_symmetric_num_cols = []\n",
    "test_symmetric_num_cols = []\n",
    "def get_symmetric_cols(skew_bin_value):\n",
    "    train_symmetric_num_cols = train_skewness_df.loc[train_skewness_df['skew_bin'] == skew_bin_value]\n",
    "    train_symmetric_num_cols = train_symmetric_num_cols['columns'].to_list()\n",
    "    test_symmetric_num_cols = test_skewness_df.loc[test_skewness_df['skew_bin'] == skew_bin_value]\n",
    "    test_symmetric_num_cols = test_symmetric_num_cols['columns'].to_list()\n",
    "    return print('train_symmetric_num_cols: {}\\n{} \\ntest_symmetric_num_cols: \\n{}'.format(type(train_symmetric_num_cols),train_symmetric_num_cols,test_symmetric_num_cols))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "get_symmetric_cols('symmetric')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "tags": []
   },
   "source": [
    "# asymmetric numerical columns with missing values.\n",
    "# Missing values will be replaced with median.\n",
    "asymmetric_num_cols = train_skewness_df.loc[train_skewness_df['skew_bin'] != 'symmetric']\n",
    "asymmetric_num_cols = asymmetric_num_cols['columns'].to_list()\n",
    "asymmetric_num_cols"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# function to extract asymmetric/symmetric columns which takes skew_bin value as input parameter\n",
    "train_asymmetric_num_cols = {}\n",
    "test_asymmetric_num_cols = {}\n",
    "def get_asymmetric_cols(skew_bin_value):\n",
    "    train_asymmetric_num_cols = train_skewness_df.loc[train_skewness_df['skew_bin'] != skew_bin_value]\n",
    "    train_asymmetric_num_cols = train_asymmetric_num_cols['columns'].to_list()\n",
    "    test_asymmetric_num_cols = test_skewness_df.loc[test_skewness_df['skew_bin'] != skew_bin_value]\n",
    "    test_asymmetric_num_cols = test_asymmetric_num_cols['columns'].to_list()\n",
    "    #return print('train_asymmetric_num_cols: \\n{} \\ntest_asymmetric_num_cols: \\n{}'.format(train_asymmetric_num_cols,test_asymmetric_num_cols))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "get_asymmetric_cols('symmetric')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Imputing missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "source": [
    "for i in symmetric_num_cols:\n",
    "    mean = train_df_unqique[i].mean()\n",
    "    print(mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "source": [
    "for i in asymmetric_num_cols:\n",
    "    median = train_df_unqique[i].median()\n",
    "    print(median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_df.isna().sum().sum())\n",
    "print(test_df.isna().sum().sum())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "tags": []
   },
   "source": [
    "# impute mean or median for missing values\n",
    "# mean will not be used on skewed data as outliers have significant impact on mean.\n",
    "\n",
    "# mean imputation\n",
    "# train set\n",
    "for i in train_symmetric_num_cols:\n",
    "    mean = train_df[i].mean()\n",
    "    train_df[i].fillna(mean, inplace=True)\n",
    "\n",
    "# test set\n",
    "for i in test_symmetric_num_cols:\n",
    "    mean = test_df[i].mean()\n",
    "    test_df[i].fillna(mean, inplace=True)\n",
    "    \n",
    "    \n",
    "# median imputation\n",
    "# train set\n",
    "for i in train_asymmetric_num_cols:\n",
    "    median = train_df[i].median()\n",
    "    train_df[i].fillna(mean, inplace=True)\n",
    "\n",
    "# test set\n",
    "for i in test_asymmetric_num_cols:\n",
    "    median = test_df[i].median()\n",
    "    test_df[i].fillna(mean, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function takes dataframe and imputation method as input parameters\n",
    "def missing_value_imputation(df1, df2):\n",
    "    # mean imputation\n",
    "    for i in train_symmetric_num_cols:\n",
    "        mean = df1[i].mean()\n",
    "        df1[i].fillna(mean, inplace=True)\n",
    "    for i in test_symmetric_num_cols:\n",
    "        mean = df2[i].mean()\n",
    "        df2[i].fillna(mean, inplace=True)\n",
    "\n",
    "    # median imputation\n",
    "    for i in train_asymmetric_num_cols:\n",
    "        median = df1[i].median()\n",
    "        df1[i].fillna(median, inplace=True)\n",
    "    for i in test_asymmetric_num_cols:\n",
    "        median = df2[i].median()\n",
    "        df2[i].fillna(median, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_value_imputation(train_df, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_df.isna().sum().sum())\n",
    "print(test_df.isna().sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Debt Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Explore the top 2,500 locations where the percentage of households with a second mortgage is the highest and percent ownership is above 10 percent. Visualize using geo-map. You may keep the upper limit for the percent of households with a second mortgage to 50 percent\n",
    "- Use the following bad debt equation:\n",
    "    - Bad Debt = P (Second Mortgage ∩ Home Equity Loan)\n",
    "    - Bad Debt = second_mortgage + home_equity - home_equity_second_mortgage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandasql import sqldf\n",
    "q1 = 'select state,place,pct_own,second_mortgage,lat,lng from train_df where pct_own >0.10 and second_mortgage <0.5 order by second_mortgage DESC LIMIT 2500;'\n",
    "pysqldf = lambda q: sqldf(q, globals())\n",
    "train_df_location_mort_pct = pysqldf(q1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_location_mort_pct.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install vega_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import state boundaries\n",
    "from vega_datasets import data\n",
    "\n",
    "states = alt.topo_feature(data.us_10m.url, feature='states')\n",
    "\n",
    "background = alt.Chart(states).mark_geoshape(\n",
    "    fill='lightgray',\n",
    "    stroke='white'\n",
    ").project('albersUsa').properties(\n",
    "    width=500,\n",
    "    height=300\n",
    ")\n",
    "\n",
    "points = alt.Chart(train_df_location_mort_pct).mark_circle().encode(\n",
    "    longitude='lng:Q',\n",
    "    latitude='lat:Q',\n",
    "    size=alt.value(10),\n",
    "    tooltip=['place', 'state', 'pct_own'],\n",
    "    color='state'\n",
    ").project(\n",
    "    \"albersUsa\"\n",
    ").properties(\n",
    "    width=500,\n",
    "    height=400\n",
    ")\n",
    "\n",
    "background + points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Use the following bad debt equation: Bad Debt = P (Second Mortgage ∩ Home Equity Loan) Bad Debt = second_mortgage + home_equity - home_equity_second_mortgage c) Create pie charts to show overall debt and bad debt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['bad_debt'] = train_df['second_mortgage'] + train_df['home_equity'] - train_df['home_equity_second_mortgage']\n",
    "\n",
    "# test set\n",
    "test_df['bad_debt'] = test_df['second_mortgage'] + test_df['home_equity'] - test_df['home_equity_second_mortgage']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df['bad_debt_bins'] = pd.cut(train_df['bad_debt'], bins=[0, 0.1, 1], labels=['less than 50%', 'greater than or equal to 50%'])\n",
    "train_df.groupby(['bad_debt_bins']).size().plot(kind='pie', subplots=True, startangle=90, autopct='%1.1f%%')\n",
    "plt.axis('equal')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# test set\n",
    "test_df['bad_debt_bins'] = pd.cut(test_df['bad_debt'], bins=[0, 0.1, 1], labels=['less than 50%', 'greater than or equal to 50%'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Create box and whisker plot and analyze the distribution for 2nd mortgage, home equity, good debt, and bad debt for different cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Georgia city comparison dataframe\n",
    "duluth_df = train_df.loc[(train_df['city'] == 'Duluth') & (train_df['state'] == 'Georgia')]\n",
    "lilburn_df = train_df.loc[(train_df['city'] == 'Lilburn') & (train_df['state'] == 'Georgia')]\n",
    "ga_duluth_lilburn_df = pd.concat([duluth_df, lilburn_df])\n",
    "ga_duluth_lilburn_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = alt.Chart(ga_duluth_lilburn_df).mark_boxplot().encode(\n",
    "    x='city',\n",
    "    y='second_mortgage'\n",
    ").properties(\n",
    "    width=200,\n",
    "    height=400\n",
    ")\n",
    "\n",
    "base | base.encode(y='home_equity') | base.encode(y='debt') | base.encode(y='bad_debt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### Create a collated income distribution chart for family income, house hold income, and remaining income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Family income distribution\n",
    "sns.displot(train_df['family_mean'])\n",
    "plt.title('Family income distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Household income distribution\n",
    "sns.displot(train_df['hi_mean'])\n",
    "plt.title('Household income distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remaining income\n",
    "sns.displot((train_df['family_mean']) - (train_df['hi_mean']))\n",
    "plt.title('Remaining income distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### Perform EDA and come out with insights into population density and age. You may have to derive new fields (make sure to weight averages for accurate measurements): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Population density\n",
    "base = alt.Chart(train_df).mark_circle().encode(\n",
    "    x='pop',\n",
    "    y='count()'\n",
    ").properties(\n",
    "    width=400,\n",
    "    height=400\n",
    ")\n",
    "\n",
    "base | base.encode(x='female_pop') | base.encode(x='male_pop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Age density\n",
    "base = alt.Chart(train_df).mark_circle().encode(\n",
    "    x='female_age_mean',\n",
    "    y='female_age_samples'\n",
    ").properties(\n",
    "    width=400,\n",
    "    height=400\n",
    ")\n",
    "\n",
    "base | base.encode(x='male_age_mean', y='male_age_samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Use pop and ALand variables to create a new field called population density. Use male_age_median, female_age_median, male_pop, and female_pop to create a new field called median age."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# derive population density https://www.nationalgeographic.org/encyclopedia/population-density/\n",
    "train_df['pop_density'] = train_df['pop'] / train_df['ALand']\n",
    "\n",
    "# median age\n",
    "train_df['median_age'] = (train_df['male_age_median'] + train_df['female_age_median']) / 2\n",
    "\n",
    "# test set\n",
    "test_df['pop_density'] = test_df['pop'] / test_df['ALand']\n",
    "# median age\n",
    "test_df['median_age'] = (test_df['male_age_median'] + test_df['female_age_median']) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Population density\n",
    "base = alt.Chart(train_df).mark_line().encode(\n",
    "    x='pop',\n",
    "    y='pop_density'\n",
    ").properties(\n",
    "    width=400,\n",
    "    height=400\n",
    ")\n",
    "\n",
    "base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combined age density\n",
    "base = alt.Chart(train_df).mark_line().encode(\n",
    "    x='median_age',\n",
    "    y='pop_density'\n",
    ").properties(\n",
    "    width=400,\n",
    "    height=400\n",
    ")\n",
    "\n",
    "base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create bins for population into a new variable by selecting appropriate class interval so that the number of categories don’t exceed 5 for the ease of analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['low', 'medium', 'high']\n",
    "\n",
    "# train set\n",
    "train_df['pop_bin'] = pd.cut(train_df['pop'], bins=3, labels=labels)\n",
    "\n",
    "# test set\n",
    "test_df['pop_bin'] = pd.cut(test_df['pop'], bins=3, labels=labels)\n",
    "train_df.shape, test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[['married', 'separated', 'divorced', 'pop_bin']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# married, separated, divorces vs pop_bin density\n",
    "base = alt.Chart(train_df).mark_bar().encode(\n",
    "    x='pop_bin',\n",
    "    y='married'\n",
    ").properties(\n",
    "    width=400,\n",
    "    height=400\n",
    ")\n",
    "\n",
    "base | base.encode(y='separated') | base.encode(y='divorced')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We can see that there are more married people in the high and medium population bins and equal numbers in the low bin across the three fields; Married, Separated and Divorced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Please detail your observations for rent as a percentage of income at an overall level, and for different states.\n",
    "Perform correlation analysis for all the relevant variables by creating a heatmap. Describe your findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# state family mean\n",
    "state_family_income_avg = train_df.groupby(by='state')['family_mean'].agg(['mean'])\n",
    "state_family_income_avg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# state rent mean\n",
    "state_rent_avg = train_df.groupby(by='state')['rent_mean'].agg(['mean'])\n",
    "state_rent_avg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rent_income_perc = state_rent_avg['mean']/state_family_income_avg['mean']\n",
    "rent_income_perc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# national rent percentage of family income\n",
    "sum(train_df['rent_mean']) / sum(train_df['family_mean'])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The state average and national average rent percentage of family income are comparable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### The economic multivariate data has a significant number of measured variables. The goal is to find where the measured variables depend on a number of smaller unobserved common factors or latent variables. \n",
    "- Each variable is assumed to be dependent upon a linear combination of the common factors, and the coefficients are known as loadings. Each measured variable also includes a component due to independent random variability, known as “specific variance” because it is specific to one variable. Obtain the common factors and then plot the loadings. Use factor analysis to find latent variables in our dataset and gain insight into the linear relationships in the data.\n",
    "\n",
    "Following are the list of latent variables:\n",
    "- Highschool graduation rates\n",
    "- Median population age\n",
    "- Second mortgage statistics\n",
    "- Percent own\n",
    "- Bad debt expense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "###### Factor Analysis\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.FactorAnalysis.html\n",
    "- https://factor-analyzer.readthedocs.io/en/latest/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import FactorAnalysis\n",
    "from factor_analyzer import FactorAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install factor_analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "f_analyzer = FactorAnalyzer(n_factors=5)\n",
    "f_analyzer.fit_transform(train_df.select_dtypes(exclude=('object', 'category')))\n",
    "f_analyzer.loadings_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.columns"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "y_corr = train_df.corr()[['hc_mortgage_mean']]\n",
    "y_corr.head(2)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "y_corr.loc[y_corr['hc_mortgage_mean'] >= 0.6]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "y_corr.loc[y_corr['hc_mortgage_mean'] <= -0.6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_columns = ['ALand', 'hi_mean', 'family_mean', 'second_mortgage', 'home_equity', \n",
    "                   'married', 'separated', 'divorced', 'pop', 'bad_debt', 'median_age', 'hc_mortgage_mean']\n",
    "len(relevant_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corr matrix\n",
    "relevant_cols_corr = train_df[relevant_columns].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "sns.heatmap(relevant_cols_corr, annot=True, cmap='RdYlBu_r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "In the preceeding heatmap, we can see there is a strong positive correlation between variables that have orange or red squares. There is little or no correlation between variables with pale-colored squares."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Build a linear Regression model to predict the total monthly expenditure for home mortgages loan. Column **hc_mortgage_mean** is predicted variable. This is the mean monthly mortgage and owner costs of specified geographical location. *Note*: Exclude loans from prediction model which have NaN (Not a Number) values for hc_mortgage_mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['hc_mortgage_mean'].isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['hc_mortgage_mean'].isna().sum().sum()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# create dummy variables for categorical data\n",
    "train_df_tran = pd.get_dummies(train_df)\n",
    "test_df_tran = pd.get_dummies(test_df)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "train_df_tran.shape, test_df_tran.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib as mpl\n",
    "\n",
    "#import seaborn as sns\n",
    "\n",
    "#import matplotlib.pyplot as plt\n",
    "\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "import statsmodels.graphics.api as smg\n",
    "\n",
    "#import pandas as pd\n",
    "\n",
    "#import numpy as np\n",
    "\n",
    "import patsy\n",
    "\n",
    "from statsmodels.graphics.correlation import plot_corr\n",
    "\n",
    "#from sklearn.model_selection import train_test_split\n",
    "\n",
    "plt.style.use('seaborn')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "['ALand', 'hi_mean', 'family_mean', 'second_mortgage', 'home_equity', \n",
    "                   'married', 'separated', 'divorced', 'pop', 'bad_debt', 'median_age', 'hc_mortgage_mean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiple linear regression using Statmodels Formula Api\n",
    "multiLinearModel = smf.ols(formula= 'hc_mortgage_mean ~ ALand + hi_mean + family_mean + second_mortgage + home_equity + married + separated + divorced + pop + bad_debt',data=train_df)\n",
    "multiLinearModResult = multiLinearModel.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(multiLinearModResult.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "train_df_tran.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data for modeling\n",
    "train_cols_to_drop = []\n",
    "for i in train_df.columns:\n",
    "    if ((train_df[i].dtype != 'float64') & (train_df[i].dtype != 'int64')):\n",
    "        train_cols_to_drop.append(i)\n",
    "train_cols_to_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data for modeling\n",
    "test_cols_to_drop = []\n",
    "for i in test_df.columns:\n",
    "    if ((test_df[i].dtype != 'float64') & (test_df[i].dtype != 'int64')):\n",
    "        test_cols_to_drop.append(i)\n",
    "test_cols_to_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define data sets for model\n",
    "\n",
    "x_train = train_df.drop(['hc_mortgage_mean', 'COUNTYID','STATEID','state', 'state_ab', 'city', 'place', 'type', 'zip_code', 'area_code', 'bad_debt_bins', 'pop_bin'], axis=1)\n",
    "y_train = train_df['hc_mortgage_mean']\n",
    "\n",
    "# test set\n",
    "x_test = test_df.drop(['hc_mortgage_mean', 'COUNTYID','STATEID','state', 'state_ab', 'city', 'place', 'type', 'zip_code', 'area_code', 'bad_debt_bins', 'pop_bin'], axis=1)\n",
    "y_test = test_df['hc_mortgage_mean']\n",
    "\n",
    "from sklearn import preprocessing\n",
    "minmaxScaler = preprocessing.MinMaxScaler()\n",
    "x_train_tran = pd.DataFrame(minmaxScaler.fit_transform(x_train))\n",
    "x_test_tran = pd.DataFrame(minmaxScaler.fit_transform(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# benchmark model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# fit a linear regression model to the data\n",
    "bench_mark_lr_model = LinearRegression()\n",
    "bench_mark_lr_model.fit(x_train_tran, y_train)\n",
    "bench_mark_lr_model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bench_mark_lr_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dimensionality reduction using pca\n",
    "from sklearn.decomposition import PCA\n",
    "import time\n",
    "t0 = time.time()\n",
    "pca = PCA().fit(x_train_tran)\n",
    "t1 = time.time()\n",
    "print('PCA fitting time:', round(t1-t0, 3), 's')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# predict outcomes of the training and testing sets using .predict()\n",
    "preds_train = bench_mark_lr_model.predict(x_train_tran)\n",
    "preds_test = bench_mark_lr_model.predict(x_test_tran)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# calculate mean squared erro on training set and print it's value\n",
    "from sklearn.metrics import mean_squared_error as mse"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# fit a linear regression model to the data\n",
    "bench_mark_lr_model = LinearRegression()\n",
    "bench_mark_lr_model.fit(x_train_tran, y_train)\n",
    "bench_mark_lr_model.coef_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
